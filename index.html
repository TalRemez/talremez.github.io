<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Tal Remez</title>
    <meta property="og:title">
    <meta property="og:type" content="article">
    <!-- FIXME(shillingford): add final URL -->
    <meta property="og:url" content="">
    <meta property="og:image" content="images/vdtts_teaser.webp">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@300&family=Roboto:wght@300&display=swap" rel="preload" as="style">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@300&family=Roboto:wght@300&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="main">

      <h1 class="title">
        Tal Remez Ph.D
      </h1>
      <h3>
        talremez at gmail dot com
      </h3>

      <table width="900" align="center" border="0" cellpadding="0">
          <tbody><tr>
            <td width="25%" valign="center">
              <img src="./images/profilepic.webp" alt="visually driven tts" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="top">
              I am part of Google Research and Machine Intelligence (RMI). Before that I was an intern at Google, General Motors and Microsoft. I received my PhD and MSc from Tel-Aviv University, where I was advised by Prof. Alex Bronstein and Prof. Shai Avidan. I recieved my MBA and BSc in Applied Physics from the Hebrew University under Amirim.
              <br><br>
              My research interests include: visual perception and computational photography as well as audio-visual applications for speech and audio enhancement. 
              <br><br>
              <b>I’m searching for my next big challenge where I can make a significant impact.</b>
            </td>

          </tr></tbody></table>      
        
      <br><br><br><br>
      <hr>
      
      <h2 class="title">
        Publications
      </h2>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/vdtts_teaser.webp" alt="visually driven tts" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/2111.10139">
                  <heading>More than Words: In-the-Wild Visually-Driven Prosody for Text-to-Speech</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Michael Hassid, Michelle Tadmor Ramanovich, Brendan Shillingford, Miaosen Wang, Ye Jia, Tal Remez</em>
                  <br><br>
                  Under review CVPR 2022
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/audioscope2.png" alt="Improving On-Screen Sound Separation for Open Domain Videos with Audio-Visual Self-attention" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/2106.09669">
                  <heading>Improving On-Screen Sound Separation for Open Domain Videos with Audio-Visual Self-attention</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Efthymios Tzinis, Scott Wisdom, Tal Remez, John R Hershey</em>
                  <br><br>
                  Under review CVPR 2022
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/translatotron2.png" alt="Translatotron 2: Robust direct speech-to-speech translation" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/2107.08661">
                  <heading>Translatotron 2: Robust direct speech-to-speech translation</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Ye Jia, Michelle Tadmor Ramanovich, Tal Remez, Roi Pomerantz</em>
                  <br><br>
                  Under review ICLR 2022
                  <br><br>

                  <a href="https://ai.googleblog.com/2021/09/high-quality-robust-and-responsible.html">Website</a>
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/audioscope.png" alt="Into the Wild with AudioScope: Unsupervised Audio-Visual Separation of On-Screen Sounds" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/2011.01143">
                  <heading>Into the Wild with AudioScope: Unsupervised Audio-Visual Separation of On-Screen Sounds</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Efthymios Tzinis, Scott Wisdom, Aren Jansen, Shawn Hershey, Tal Remez, Daniel P. W. Ellis, John R. Hershey</em>
                  <br><br>
                  ICLR 2021
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/learning_to_segment.png" alt="Learning to Segment via Cut-and-Paste" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Tal_Remez_Learning_to_Segment_ECCV_2018_paper.pdf">
                  <heading>Learning to Segment via Cut-and-Paste</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Tal Remez, Jonathan Huang, Matthew Brown</em>
                  <br><br>
                  ECCV 2018
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/denoisenet.png" alt="Class-Aware Fully-Convolutional Gaussian and Poisson Denoising
" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1808.06562">
                  <heading>Class-Aware Fully-Convolutional Gaussian and Poisson Denoising
</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Tal Remez, Or Litany, Raja Giryes, Alex M. Bronstein</em>
                  <br><br>
                  TIP 2018
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>      
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/fmnet.png" alt="Deep Functional Maps: Structured Prediction for Dense Shape Correspondence" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1704.08686">
                  <heading>Deep Functional Maps: Structured Prediction for Dense Shape Correspondence</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Or Litany, Tal Remez, Emanuele Rodolà, Alex M. Bronstein, Michael M. Bronstein</em>
                  <br><br>
                  ICCV 2017
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/class_aware.jpeg" alt="Deep Class Aware Image Denoising" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1701.01698">
                  <heading>Deep Class Aware Image Denoising</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Tal Remez, Or Litany, Raja Giryes and Alex M. Bronstein</em>
                  <br><br>
                  ICIP 2017
                  <br><br>
                  <a href="https://github.com/TalRemez/deep_class_aware_denoising">GitHub</a>
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/low_light.jpeg" alt="
Deep Convolutional Denoising of Low-Light Images" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1701.01687">
                  <heading>Deep Convolutional Denoising of Low-Light Images</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Tal Remez, Or Litany, Raja Giryes and Alex M. Bronstein</em>
                  <br><br>
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/cloud.png" alt="Cloud Dictionary: Sparse Coding and Modeling for Point Clouds" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1612.04956">
                  <heading>Cloud Dictionary: Sparse Coding and Modeling for Point Clouds</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Or Litany*, Tal Remez*, Alex Bronstein</em>
                  <br><br>
                  SPARS 2017
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/asist.png" alt="ASIST: Automatic Semantically Invariant Scene Transformation" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1512.01515">
                  <heading>ASIST: Automatic Semantically Invariant Scene Transformation</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Or Litany, Tal Remez, Daniel Freedman, Lior Shapira, Alex Bronstein, Ran Gal</em>
                  <br><br>
                  CVIU 2017
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/cartmen.png" alt="A picture is worth a billion bits: Real-time image reconstruction from dense binary threshold pixels" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1510.04601">
                  <heading>A picture is worth a billion bits: Real-time image reconstruction from dense binary threshold pixels</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Tal Remez,Or Litany, Alex Bronstein</em>
                  <br><br>
                  ICCP 2016 Oral
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/lena.png" alt="Image reconstruction from dense binary pixels" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1510.04601">
                  <heading>Image reconstruction from dense binary pixels</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Or Litany*, Tal Remez*, Alex Bronstein</em>
                  <br><br>
                  SPARS 2015
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
    </body> 
</html>