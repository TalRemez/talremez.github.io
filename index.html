<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Tal Remez</title>
    <meta property="og:title">
    <meta property="og:type" content="article">
    <!-- FIXME(shillingford): add final URL -->
    <meta property="og:url" content="">
    <meta property="og:image" content="images/vdtts_teaser.webp">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="google-site-verification" content="DBam0wHetCHKLyRf6Hwi5UM7sj_Ilaqe_NBwOVKiPIc" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@300&family=Roboto:wght@300&display=swap" rel="preload" as="style">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@300&family=Roboto:wght@300&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="main">

      <h1 class="title">
        Tal Remez Ph.D
      </h1>
      <h3>
        talremez at gmail dot com
      </h3>

      <table width="900" align="center" border="0" cellpadding="0">
          <tbody><tr>
            <td width="25%" valign="center">
              <img src="./images/profilepic.webp" alt="tal" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="top">
I am an AI and machine learning researcher with a PhD and experience across a variety of fields, including large language models (LLMs), optimization, visual perception, computational photography, and audio-visual methods for speech enhancement. My work has focused on applications of LLMs in audio and music generation, as well as text and code generation. Recently, I’ve explored techniques like flow matching and diffusion in latent text embeddings for tasks like chain-of-thought reasoning.

With a background at Facebook AI Research (FAIR) and Google Research, I’ve led projects that pushed the boundaries of AI, including on-device audio-visual speech separation and advancements in LLMs. I’m now looking for my next challenge where I can apply my skills and collaborate with a talented team to drive meaningful outcomes.
            </td>

          </tr></tbody></table>      
        
      <br><br><br><br>
      <hr>
      
      <h2 class="title">
        Publications
      </h2>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/dfm.png" alt="dfm" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/pdf/2407.15595">
                  <heading>Discrete flow matching</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky TQ Chen, Gabriel Synnaeve, Yossi Adi, Yaron Lipman</em>
                  <br><br>
                  NeurIPS 2024
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/larger.png" alt="larger" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/2404.00725">
                  <heading>The Larger the Better? Improved LLM Code-Generation via Budget Reallocation</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Michael Hassid*, Tal Remez*, Jonas Gehring, Roy Schwartz, Yossi Adi</em>
                  <br><br>
                  COLM 2024
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/musicgen.png" alt="musicgen" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/94b472a1842cd7c56dcb125fb2765fbd-Paper-Conference.pdf">
                  <heading>Simple and controllable music generation</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez</em>
                  <br><br>
                  NeurIPS 2024
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/twist.png" alt="twist" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/c859b99b5d717c9035e79d43dfd69435-Paper-Conference.pdf">
                  <heading>Textually pretrained speech language models</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Michael Hassid*, Tal Remez*, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, Roy Schwartz, Yossi Adi</em>
                  <br><br>
                  NeurIPS 2024
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/revise.png" alt="Revise" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hsu_ReVISE_Self-Supervised_Speech_Resynthesis_With_Visual_Input_for_Universal_and_CVPR_2023_paper.pdf">
                  <heading>Revise: Self-supervised speech resynthesis with visual input for universal and generalized speech regeneration</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Wei-Ning Hsu, Tal Remez, Bowen Shi, Jacob Donley, Yossi Adi</em>
                  <br><br>
                  CVPR 2023
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/vdtts_teaser.webp" alt="visually driven tts" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/2111.10139">
                  <heading>More than Words: In-the-Wild Visually-Driven Prosody for Text-to-Speech</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Michael Hassid, Michelle Tadmor Ramanovich, Brendan Shillingford, Miaosen Wang, Ye Jia, Tal Remez</em>
                  <br><br>
                  CVPR 2022
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/audioscope2.png" alt="Improving On-Screen Sound Separation for Open Domain Videos with Audio-Visual Self-attention" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/2106.09669">
                  <heading>Improving On-Screen Sound Separation for Open Domain Videos with Audio-Visual Self-attention</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Efthymios Tzinis, Scott Wisdom, Tal Remez, John R Hershey</em>
                  <br><br>
                  CVPR 2022
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/translatotron2.png" alt="Translatotron 2: Robust direct speech-to-speech translation" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/2107.08661">
                  <heading>Translatotron 2: Robust direct speech-to-speech translation</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Ye Jia, Michelle Tadmor Ramanovich, Tal Remez, Roi Pomerantz</em>
                  <br><br>
                  ICML 2022
                  <br><br>

                  <a href="https://ai.googleblog.com/2021/09/high-quality-robust-and-responsible.html">Website</a>
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/audioscope.png" alt="Into the Wild with AudioScope: Unsupervised Audio-Visual Separation of On-Screen Sounds" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/2011.01143">
                  <heading>Into the Wild with AudioScope: Unsupervised Audio-Visual Separation of On-Screen Sounds</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Efthymios Tzinis, Scott Wisdom, Aren Jansen, Shawn Hershey, Tal Remez, Daniel P. W. Ellis, John R. Hershey</em>
                  <br><br>
                  ICLR 2021
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/learning_to_segment.png" alt="Learning to Segment via Cut-and-Paste" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Tal_Remez_Learning_to_Segment_ECCV_2018_paper.pdf">
                  <heading>Learning to Segment via Cut-and-Paste</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Tal Remez, Jonathan Huang, Matthew Brown</em>
                  <br><br>
                  ECCV 2018
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/denoisenet.png" alt="Class-Aware Fully-Convolutional Gaussian and Poisson Denoising
" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1808.06562">
                  <heading>Class-Aware Fully-Convolutional Gaussian and Poisson Denoising
</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Tal Remez, Or Litany, Raja Giryes, Alex M. Bronstein</em>
                  <br><br>
                  TIP 2018
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>      
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/fmnet.png" alt="Deep Functional Maps: Structured Prediction for Dense Shape Correspondence" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1704.08686">
                  <heading>Deep Functional Maps: Structured Prediction for Dense Shape Correspondence</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Or Litany, Tal Remez, Emanuele Rodolà, Alex M. Bronstein, Michael M. Bronstein</em>
                  <br><br>
                  ICCV 2017
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/class_aware.jpeg" alt="Deep Class Aware Image Denoising" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1701.01698">
                  <heading>Deep Class Aware Image Denoising</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Tal Remez, Or Litany, Raja Giryes and Alex M. Bronstein</em>
                  <br><br>
                  ICIP 2017
                  <br><br>
                  <a href="https://github.com/TalRemez/deep_class_aware_denoising">GitHub</a>
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/low_light.jpeg" alt="
Deep Convolutional Denoising of Low-Light Images" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1701.01687">
                  <heading>Deep Convolutional Denoising of Low-Light Images</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Tal Remez, Or Litany, Raja Giryes and Alex M. Bronstein</em>
                  <br><br>
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/cloud.png" alt="Cloud Dictionary: Sparse Coding and Modeling for Point Clouds" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1612.04956">
                  <heading>Cloud Dictionary: Sparse Coding and Modeling for Point Clouds</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Or Litany*, Tal Remez*, Alex Bronstein</em>
                  <br><br>
                  SPARS 2017
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/asist.png" alt="ASIST: Automatic Semantically Invariant Scene Transformation" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1512.01515">
                  <heading>ASIST: Automatic Semantically Invariant Scene Transformation</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Or Litany, Tal Remez, Daniel Freedman, Lior Shapira, Alex Bronstein, Ran Gal</em>
                  <br><br>
                  CVIU 2017
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/cartmen.png" alt="A picture is worth a billion bits: Real-time image reconstruction from dense binary threshold pixels" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1510.04601">
                  <heading>A picture is worth a billion bits: Real-time image reconstruction from dense binary threshold pixels</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Tal Remez,Or Litany, Alex Bronstein</em>
                  <br><br>
                  ICCP 2016 Oral
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/lena.png" alt="Image reconstruction from dense binary pixels" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1510.04601">
                  <heading>Image reconstruction from dense binary pixels</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Or Litany*, Tal Remez*, Alex Bronstein</em>
                  <br><br>
                  SPARS 2015
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
    </body> 
</html>
