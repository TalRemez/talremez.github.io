<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Tal Remez</title>
    <meta property="og:title">
    <meta property="og:type" content="article">
    <!-- FIXME(shillingford): add final URL -->
    <meta property="og:url" content="">
    <meta property="og:image" content="images/vdtts_teaser.webp">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="google-site-verification" content="DBam0wHetCHKLyRf6Hwi5UM7sj_Ilaqe_NBwOVKiPIc" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@300&family=Roboto:wght@300&display=swap" rel="preload" as="style">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@300&family=Roboto:wght@300&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="main">

      <h1 class="title">
        Tal Remez Ph.D
      </h1>
      <h3>
        talremez at gmail dot com
      </h3>

      <table width="900" align="center" border="0" cellpadding="0">
          <tbody><tr>
            <td width="25%" valign="center">
              <img src="./images/profilepic.webp" alt="tal" width="200" height="" style="border-style: none">
            </td>
            <td width="75%" valign="top">
I am an AI researcher with a PhD in machine learning, specializing in the development of large-scale foundation models and innovative architectures like world models. My expertise spans multi-modal large language models (LLMs) encompassing video, audio, and textual data, as well as core areas like optimization, visual perception, and computational photography.

My latest work at Amazon and FAIR has centered on pushing the boundaries of these models, including leading a project on large-scale multi-modal foundation models for sport understanding and developing a "Code World Model." This work includes high-impact applications of multi-modal LLMs in audio/music generation, text/code generation, and advanced research into techniques like flow matching and discrete flow matching for tasks such as text continuation and chain-of-thought reasoning.

            </td>

          </tr></tbody></table>      
        
      <br><br><br><br>
      <hr>
      
      <h2 class="title">
        Publications
      </h2>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/dfm.png" alt="dfm" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/pdf/2407.15595">
                  <heading>Discrete flow matching</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky TQ Chen, Gabriel Synnaeve, Yossi Adi, Yaron Lipman</em>
                  <br><br>
                  NeurIPS 2024
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/larger.png" alt="larger" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/2404.00725">
                  <heading>The Larger the Better? Improved LLM Code-Generation via Budget Reallocation</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Michael Hassid*, Tal Remez*, Jonas Gehring, Roy Schwartz, Yossi Adi</em>
                  <br><br>
                  COLM 2024
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/musicgen.png" alt="musicgen" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/94b472a1842cd7c56dcb125fb2765fbd-Paper-Conference.pdf">
                  <heading>Simple and controllable music generation</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez</em>
                  <br><br>
                  NeurIPS 2024
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/twist.png" alt="twist" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/c859b99b5d717c9035e79d43dfd69435-Paper-Conference.pdf">
                  <heading>Textually pretrained speech language models</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Michael Hassid*, Tal Remez*, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, Roy Schwartz, Yossi Adi</em>
                  <br><br>
                  NeurIPS 2024
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/revise.png" alt="Revise" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hsu_ReVISE_Self-Supervised_Speech_Resynthesis_With_Visual_Input_for_Universal_and_CVPR_2023_paper.pdf">
                  <heading>Revise: Self-supervised speech resynthesis with visual input for universal and generalized speech regeneration</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Wei-Ning Hsu, Tal Remez, Bowen Shi, Jacob Donley, Yossi Adi</em>
                  <br><br>
                  CVPR 2023
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/vdtts_teaser.webp" alt="visually driven tts" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/2111.10139">
                  <heading>More than Words: In-the-Wild Visually-Driven Prosody for Text-to-Speech</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Michael Hassid, Michelle Tadmor Ramanovich, Brendan Shillingford, Miaosen Wang, Ye Jia, Tal Remez</em>
                  <br><br>
                  CVPR 2022
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/audioscope2.png" alt="Improving On-Screen Sound Separation for Open Domain Videos with Audio-Visual Self-attention" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/2106.09669">
                  <heading>Improving On-Screen Sound Separation for Open Domain Videos with Audio-Visual Self-attention</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Efthymios Tzinis, Scott Wisdom, Tal Remez, John R Hershey</em>
                  <br><br>
                  CVPR 2022
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/translatotron2.png" alt="Translatotron 2: Robust direct speech-to-speech translation" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/2107.08661">
                  <heading>Translatotron 2: Robust direct speech-to-speech translation</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Ye Jia, Michelle Tadmor Ramanovich, Tal Remez, Roi Pomerantz</em>
                  <br><br>
                  ICML 2022
                  <br><br>

                  <a href="https://ai.googleblog.com/2021/09/high-quality-robust-and-responsible.html">Website</a>
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/audioscope.png" alt="Into the Wild with AudioScope: Unsupervised Audio-Visual Separation of On-Screen Sounds" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/2011.01143">
                  <heading>Into the Wild with AudioScope: Unsupervised Audio-Visual Separation of On-Screen Sounds</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Efthymios Tzinis, Scott Wisdom, Aren Jansen, Shawn Hershey, Tal Remez, Daniel P. W. Ellis, John R. Hershey</em>
                  <br><br>
                  ICLR 2021
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/learning_to_segment.png" alt="Learning to Segment via Cut-and-Paste" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Tal_Remez_Learning_to_Segment_ECCV_2018_paper.pdf">
                  <heading>Learning to Segment via Cut-and-Paste</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Tal Remez, Jonathan Huang, Matthew Brown</em>
                  <br><br>
                  ECCV 2018
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/denoisenet.png" alt="Class-Aware Fully-Convolutional Gaussian and Poisson Denoising
" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1808.06562">
                  <heading>Class-Aware Fully-Convolutional Gaussian and Poisson Denoising
</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Tal Remez, Or Litany, Raja Giryes, Alex M. Bronstein</em>
                  <br><br>
                  TIP 2018
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>      
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/fmnet.png" alt="Deep Functional Maps: Structured Prediction for Dense Shape Correspondence" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1704.08686">
                  <heading>Deep Functional Maps: Structured Prediction for Dense Shape Correspondence</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Or Litany, Tal Remez, Emanuele Rodolà, Alex M. Bronstein, Michael M. Bronstein</em>
                  <br><br>
                  ICCV 2017
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/class_aware.jpeg" alt="Deep Class Aware Image Denoising" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1701.01698">
                  <heading>Deep Class Aware Image Denoising</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Tal Remez, Or Litany, Raja Giryes and Alex M. Bronstein</em>
                  <br><br>
                  ICIP 2017
                  <br><br>
                  <a href="https://github.com/TalRemez/deep_class_aware_denoising">GitHub</a>
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/low_light.jpeg" alt="
Deep Convolutional Denoising of Low-Light Images" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1701.01687">
                  <heading>Deep Convolutional Denoising of Low-Light Images</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Tal Remez, Or Litany, Raja Giryes and Alex M. Bronstein</em>
                  <br><br>
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/cloud.png" alt="Cloud Dictionary: Sparse Coding and Modeling for Point Clouds" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1612.04956">
                  <heading>Cloud Dictionary: Sparse Coding and Modeling for Point Clouds</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Or Litany*, Tal Remez*, Alex Bronstein</em>
                  <br><br>
                  SPARS 2017
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/asist.png" alt="ASIST: Automatic Semantically Invariant Scene Transformation" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1512.01515">
                  <heading>ASIST: Automatic Semantically Invariant Scene Transformation</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Or Litany, Tal Remez, Daniel Freedman, Lior Shapira, Alex Bronstein, Ran Gal</em>
                  <br><br>
                  CVIU 2017
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/cartmen.png" alt="A picture is worth a billion bits: Real-time image reconstruction from dense binary threshold pixels" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1510.04601">
                  <heading>A picture is worth a billion bits: Real-time image reconstruction from dense binary threshold pixels</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Tal Remez,Or Litany, Alex Bronstein</em>
                  <br><br>
                  ICCP 2016 Oral
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
      <table width="1000" align="center" border="0" cellpadding="0">
          <tbody>
            <tr>
              <td width="35%" valign="center">
                <img src="./images/lena.png" alt="Image reconstruction from dense binary pixels" width="300" height="" style="border-style: none">
              </td>
              <td width="65%" valign="center">
                <p><a href="https://arxiv.org/abs/1510.04601">
                  <heading>Image reconstruction from dense binary pixels</heading></a><br>
                  <span v-html="pub.authors"></span><br>
                  <em>Or Litany*, Tal Remez*, Alex Bronstein</em>
                  <br><br>
                  SPARS 2015
                </p>
              </td>
            </tr>
        </tbody>
      </table>
      <br><hr><br>
    </body> 
</html>
